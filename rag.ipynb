{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Test langchain integration with Llama Cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/cgebbe/git/_private/code_rag_prototype/models/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  2653.31 MiB, ( 2653.38 / 21845.34)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    41.02 MiB\n",
      "llm_load_tensors:      Metal buffer size =  2653.31 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniconda/base/envs/code_rag_application/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2000.00 MiB, ( 4654.06 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2000.00 MiB, K (f16): 1000.00 MiB, V (f16): 1000.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 4654.08 / 21845.34)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     4.41 MiB, ( 4658.47 / 21845.34)\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "llama_new_context_with_model:      Metal compute buffer size =     4.40 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     0.25 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "dirpath = Path(\"/Users/cgebbe/git/_private/code_rag_prototype/models\")\n",
    "MODEL_PATH = {\n",
    "    # codellama yielded poor results\n",
    "    \"codellama\": dirpath / \"codellama-7b.Q4_K_M.gguf\",\n",
    "    # deepseek performed pretty well on leaderboard and is fast with llama.cpp.\n",
    "    # However, it doesn't seem to work with langchain, see https://github.com/langchain-ai/langchain/issues/14593\n",
    "    \"deepseek\": dirpath / \"deepseek-coder-6.7b-instruct.Q4_K_M.gguf\",\n",
    "    # llama works somewhat\n",
    "    \"llama\": dirpath / \"llama-2-7b-chat.Q2_K.gguf\",\n",
    "}[\"llama\"]\n",
    "assert MODEL_PATH.exists()\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    temperature=0.75,\n",
    "    # max_tokens=2000,\n",
    "    n_ctx=4000,\n",
    "    n_gpu_layers=-1,\n",
    "    # n_batch=512, # should be between 1 and n_ctx ?!\n",
    "    top_p=1,\n",
    "    # f16_kv=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cgebbe/git/_private/code_rag_prototype/rag.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cgebbe/git/_private/code_rag_prototype/rag.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cgebbe/git/_private/code_rag_prototype/rag.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mWrite a fastapi app\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cgebbe/git/_private/code_rag_prototype/rag.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cgebbe/git/_private/code_rag_prototype/rag.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m llm(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a fastapi app\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"/Users/cgebbe/git/_private/elliptio_data_lake\"\n",
    "Path(repo_path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      10\n"
     ]
    }
   ],
   "source": [
    "!find {repo_path} -name '*.py' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='def rm_s3(c):\\n    s3_bucket_url = _get_s3_bucket_url()\\n    c.run(f\"aws s3 rm --recursive {s3_bucket_url}\")', metadata={'source': '/Users/cgebbe/git/_private/elliptio_data_lake/tasks.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 10 python files, but each (top-level) function and class get its own document\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    exclude=(\"__init__.py\",),\n",
    "    parser=LanguageParser(language=Language.PYTHON),\n",
    ")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='def rm_s3(c):\\n    s3_bucket_url = _get_s3_bucket_url()\\n    c.run(f\"aws s3 rm --recursive {s3_bucket_url}\")', metadata={'source': '/Users/cgebbe/git/_private/elliptio_data_lake/tasks.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "print(len(texts))\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings # Wouldn't work locally!!!\n",
    "# NOTE: How interchangable are embeddings?!\n",
    "\n",
    "# see https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n",
    "\n",
    "model_name = {\n",
    "    # this downloads all ~10 models for different quantizations (~40GB)\n",
    "    \"llama-7b\": \"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "    # pretrained model from `sentence_transformers`, see https://www.sbert.net/docs/pretrained_models.html\n",
    "    \"pretrained\": \"all-mpnet-base-v2\",\n",
    "}[\"pretrained\"]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")\n",
    "# db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work, Connection error. Likely need to setup LangSmith API key.\n",
    "# QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self) -> None:\n",
    "        self.chain = load_qa_chain(\n",
    "            llm, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT, verbose=True\n",
    "        )\n",
    "\n",
    "    def answer(self, question: str):\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "        return self.chain(\n",
    "            {\"input_documents\": docs, \"question\": question}, return_only_outputs=False\n",
    "        )\n",
    "\n",
    "\n",
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
      "Use three sentences maximum and keep the answer as concise as possible. \n",
      "from __future__ import annotations\n",
      "\n",
      "import os\n",
      "import shutil\n",
      "import typing\n",
      "from pathlib import Path, PurePosixPath\n",
      "\n",
      "from elliptio.filetypes import RemoteFileInterface\n",
      "\n",
      "if typing.TYPE_CHECKING:\n",
      "    from elliptio.metadata import Metadata\n",
      "\n",
      "\n",
      "# Code for: class LocalFile(RemoteFileInterface):\n",
      "\n",
      "import abc\n",
      "from dataclasses import dataclass\n",
      "from pathlib import Path, PurePosixPath\n",
      "\n",
      "from elliptio.metadata import Metadata\n",
      "\n",
      "\n",
      "@dataclass\n",
      "# Code for: class RemoteFileInterface(abc.ABC):\n",
      "\n",
      "def get_metadata(run_id: str) -> Metadata:\n",
      "    return Metadata(\n",
      "        artifact_id=get_id(prefix=\"artifact_\"),\n",
      "        run_id=run_id,\n",
      "        username=_get_username(),\n",
      "        hostname=_get_hostname(),\n",
      "        argv=\" \".join(sys.orig_argv),\n",
      "        creation_time=datetime.now(tz=UTC),\n",
      "        python_packages=_get_python_packages(),\n",
      "    )\n",
      "\n",
      "class Metadata:\n",
      "    username: str\n",
      "    hostname: str\n",
      "    argv: str\n",
      "    artifact_id: str\n",
      "    run_id: str\n",
      "    creation_time: datetime\n",
      "    python_packages: dict[str, str]\n",
      "\n",
      "    # TODO\n",
      "    # git hash, git diff, ...\n",
      "    # machine hostname (IP-address?)\n",
      "    # env-vars (optional due to passwords!)\n",
      "\n",
      "    # To be filled later\n",
      "    based_on: list[str] = field(default_factory=list)\n",
      "    labels: Labels = field(default_factory=Labels)\n",
      "    remote_root: str = \"\"\n",
      "    local_root: str = \"\"\n",
      "    file_relpaths: list[str] = field(default_factory=list)\n",
      "    log_relpaths: list[str] = field(default_factory=list)\n",
      "\n",
      "    # Likely useful for loading different metadata versions later on\n",
      "    version: int = 1\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import os\n",
      "import typing\n",
      "from pathlib import PurePosixPath\n",
      "\n",
      "import boto3\n",
      "import dotenv\n",
      "\n",
      "from elliptio.filetypes import interface\n",
      "\n",
      "if typing.TYPE_CHECKING:\n",
      "    from elliptio.metadata import Metadata\n",
      "import functools\n",
      "from dataclasses import dataclass\n",
      "from pathlib import Path\n",
      "\n",
      "dotenv.load_dotenv()\n",
      "\n",
      "\n",
      "@functools.lru_cache\n",
      "# Code for: def _get_bucket():\n",
      "\n",
      "\n",
      "@dataclass\n",
      "# Code for: class S3File(interface.RemoteFileInterface):\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import logging\n",
      "import os\n",
      "from contextlib import contextmanager\n",
      "from dataclasses import asdict, dataclass\n",
      "from pathlib import Path, PurePosixPath\n",
      "from typing import TYPE_CHECKING, Iterable, Iterator\n",
      "\n",
      "import dotenv\n",
      "import pandas as pd\n",
      "import pymongo.collection\n",
      "import toolz\n",
      "import yaml\n",
      "from assertpy import assert_that, soft_assertions\n",
      "from pymongo.mongo_client import MongoClient\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from elliptio.filetypes import RemoteFileInterface\n",
      "from elliptio.metadata import Labels, Metadata, get_id, get_metadata\n",
      "\n",
      "dotenv.load_dotenv()\n",
      "\n",
      "\n",
      "_LOGGER = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "# Code for: class DocumentNotFoundError(Exception):\n",
      "\n",
      "\n",
      "@dataclass\n",
      "# Code for: class Artifact:\n",
      "\n",
      "\n",
      "# Code for: class Handler:\n",
      "\n",
      "\n",
      "# Code for: def _convert_to_relative_paths(local_paths: Iterable[Path]) -> list[str]:\n",
      "\n",
      "\n",
      "# Code for: def _get_remote_url(\n",
      "\n",
      "\n",
      "# Code for: def _get_mongodb_collection(*, check_connection: bool = True):\n",
      "\n",
      "class Labels:\n",
      "    datatype: str = \"\"\n",
      "    ticket: str = \"\"\n",
      "    project: str = \"\"\n",
      "    dataset: str = \"\"\n",
      "    description: str = \"\"\n",
      "    infos: dict[str, str] = field(default_factory=dict)\n",
      "\n",
      "def mock_username(username):\n",
      "    with patch(\"elliptio.metadata._get_username\") as mock:\n",
      "        mock.return_value = username\n",
      "        yield\n",
      "Question: What automatically generated metadata does elliptio store?\n",
      "Helpful Answer:\u001b[0m\n",
      " elli"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptio stores the following automatically generated metadata for each file:\n",
      "* creation time\n",
      "* owner (the username of the user who uploaded it)\n",
      "* hostname (the hostname of the machine where the file was saved)\n",
      "* argv (the command-line arguments used to run the file)\n",
      "* python packages (the Python packages required by the file, such as NumPy or pandas)\n",
      "\n",
      "Additionally, the following metadata may be automatically generated depending on the environment:\n",
      "* labels (labels associated with the file, such as \"private\" or \"sensitive\")\n",
      "* file format (e.g., yml, yaml,json, etc.)\n",
      "* file size (in bytes)\n",
      "* last modified time (the date and time when the file was most recently modified)\n",
      "* file type (e.g., \"file\", \"directory\", \"symlink\")\n",
      "\n",
      "The metadata is retrieved from the underlying storage system (such as AWS S3 or MongoDB) using the `get_metadata` method of the `RemoteFileInterface` class.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     341.78 ms\n",
      "llama_print_timings:      sample time =      22.78 ms /   223 runs   (    0.10 ms per token,  9788.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    9741.57 ms /   223 runs   (   43.68 ms per token,    22.89 tokens per second)\n",
      "llama_print_timings:       total time =   10159.16 ms /   224 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='from __future__ import annotations\\n\\nimport os\\nimport shutil\\nimport typing\\nfrom pathlib import Path, PurePosixPath\\n\\nfrom elliptio.filetypes import RemoteFileInterface\\n\\nif typing.TYPE_CHECKING:\\n    from elliptio.metadata import Metadata\\n\\n\\n# Code for: class LocalFile(RemoteFileInterface):', metadata={'content_type': 'simplified_code', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/filetypes/local.py'}),\n",
       "  Document(page_content='import abc\\nfrom dataclasses import dataclass\\nfrom pathlib import Path, PurePosixPath\\n\\nfrom elliptio.metadata import Metadata\\n\\n\\n@dataclass\\n# Code for: class RemoteFileInterface(abc.ABC):', metadata={'content_type': 'simplified_code', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/filetypes/interface.py'}),\n",
       "  Document(page_content='def get_metadata(run_id: str) -> Metadata:\\n    return Metadata(\\n        artifact_id=get_id(prefix=\"artifact_\"),\\n        run_id=run_id,\\n        username=_get_username(),\\n        hostname=_get_hostname(),\\n        argv=\" \".join(sys.orig_argv),\\n        creation_time=datetime.now(tz=UTC),\\n        python_packages=_get_python_packages(),\\n    )', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/metadata.py'}),\n",
       "  Document(page_content='class Metadata:\\n    username: str\\n    hostname: str\\n    argv: str\\n    artifact_id: str\\n    run_id: str\\n    creation_time: datetime\\n    python_packages: dict[str, str]\\n\\n    # TODO\\n    # git hash, git diff, ...\\n    # machine hostname (IP-address?)\\n    # env-vars (optional due to passwords!)\\n\\n    # To be filled later\\n    based_on: list[str] = field(default_factory=list)\\n    labels: Labels = field(default_factory=Labels)\\n    remote_root: str = \"\"\\n    local_root: str = \"\"\\n    file_relpaths: list[str] = field(default_factory=list)\\n    log_relpaths: list[str] = field(default_factory=list)\\n\\n    # Likely useful for loading different metadata versions later on\\n    version: int = 1', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/metadata.py'}),\n",
       "  Document(page_content='from __future__ import annotations\\n\\nimport os\\nimport typing\\nfrom pathlib import PurePosixPath\\n\\nimport boto3\\nimport dotenv\\n\\nfrom elliptio.filetypes import interface\\n\\nif typing.TYPE_CHECKING:\\n    from elliptio.metadata import Metadata\\nimport functools\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\ndotenv.load_dotenv()\\n\\n\\n@functools.lru_cache\\n# Code for: def _get_bucket():\\n\\n\\n@dataclass\\n# Code for: class S3File(interface.RemoteFileInterface):', metadata={'content_type': 'simplified_code', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/filetypes/s3.py'}),\n",
       "  Document(page_content='from __future__ import annotations\\n\\nimport logging\\nimport os\\nfrom contextlib import contextmanager\\nfrom dataclasses import asdict, dataclass\\nfrom pathlib import Path, PurePosixPath\\nfrom typing import TYPE_CHECKING, Iterable, Iterator\\n\\nimport dotenv\\nimport pandas as pd\\nimport pymongo.collection\\nimport toolz\\nimport yaml\\nfrom assertpy import assert_that, soft_assertions\\nfrom pymongo.mongo_client import MongoClient\\n\\nif TYPE_CHECKING:\\n    from elliptio.filetypes import RemoteFileInterface\\nfrom elliptio.metadata import Labels, Metadata, get_id, get_metadata\\n\\ndotenv.load_dotenv()\\n\\n\\n_LOGGER = logging.getLogger(__name__)\\n\\n\\n# Code for: class DocumentNotFoundError(Exception):\\n\\n\\n@dataclass\\n# Code for: class Artifact:\\n\\n\\n# Code for: class Handler:\\n\\n\\n# Code for: def _convert_to_relative_paths(local_paths: Iterable[Path]) -> list[str]:\\n\\n\\n# Code for: def _get_remote_url(\\n\\n\\n# Code for: def _get_mongodb_collection(*, check_connection: bool = True):', metadata={'content_type': 'simplified_code', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/handler.py'}),\n",
       "  Document(page_content='class Labels:\\n    datatype: str = \"\"\\n    ticket: str = \"\"\\n    project: str = \"\"\\n    dataset: str = \"\"\\n    description: str = \"\"\\n    infos: dict[str, str] = field(default_factory=dict)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/metadata.py'}),\n",
       "  Document(page_content='def mock_username(username):\\n    with patch(\"elliptio.metadata._get_username\") as mock:\\n        mock.return_value = username\\n        yield', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/Users/cgebbe/git/_private/elliptio_data_lake/src/elliptio/metadata.py'})],\n",
       " 'question': 'What automatically generated metadata does elliptio store?',\n",
       " 'output_text': ' elliptio stores the following automatically generated metadata for each file:\\n* creation time\\n* owner (the username of the user who uploaded it)\\n* hostname (the hostname of the machine where the file was saved)\\n* argv (the command-line arguments used to run the file)\\n* python packages (the Python packages required by the file, such as NumPy or pandas)\\n\\nAdditionally, the following metadata may be automatically generated depending on the environment:\\n* labels (labels associated with the file, such as \"private\" or \"sensitive\")\\n* file format (e.g., yml, yaml,json, etc.)\\n* file size (in bytes)\\n* last modified time (the date and time when the file was most recently modified)\\n* file type (e.g., \"file\", \"directory\", \"symlink\")\\n\\nThe metadata is retrieved from the underlying storage system (such as AWS S3 or MongoDB) using the `get_metadata` method of the `RemoteFileInterface` class.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag.answer(\"What filesystems are implemented in elliptio?\") # good answer\n",
    "rag.answer(\n",
    "    \"What automatically generated metadata does elliptio store?\"\n",
    ")  # bad answer first time, very good next time?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
